# -*- coding: utf-8 -*-
"""homework-train.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qFcL-maobnyF7ICg2zV6LYPrYQ5bSP2N
"""

# Commented out IPython magic to ensure Python compatibility.
import torch
import torch.utils.data as Data
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import time,os

# Hyper Parameters 
num_classes = 25
num_epochs =10
batch_size =32
learning_rate = 0.001
split_rate=0.85
train_batch=5
PATH='mytrain.pt'
#np.random.seed(0) # 將此模型隨機參數固定
#torch.manual_seed(10)
tStart = time.time()#計時開始
#判斷是否有GPU
cuda_flag=False           
if torch.cuda.is_available():
    cuda_flag=True


def show_train_history(ep,data):
    plt.plot(data)
    plt.title('train & valid history')
    plt.ylabel(ep)
    plt.xlabel('epoch')
    plt.legend([ep],loc='upper left')
    plt.show()


# 神經網路CNN  
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.features = nn.Sequential(
            nn.Conv2d(in_channels=1 ,out_channels=256, kernel_size=3, stride=1,padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2),
            nn.Conv2d(in_channels=256 ,out_channels=64, kernel_size=3, stride=1,padding=1),
            nn.ReLU(),
            nn.MaxPool2d(kernel_size=2)
#             nn.Conv2d(in_channels=128 ,out_channels=64, kernel_size=3, stride=1,padding=1),
#             nn.ReLU(),
#             nn.MaxPool2d(kernel_size=2)  
        )
        
        self.classifier = nn.Sequential(
            nn.Dropout(),
#             nn.Linear(128*7*7,512),
#             nn.Dropout(),
#             nn.Linear(512,256),
#             nn.Dropout(),
            nn.Linear(64*7*7,num_classes)
        )
        
    def forward(self, x):
            x = self.features(x)
            x = x.view(x.size(0), -1)
            x = self.classifier(x)
            return x
          
          

# Construct a model
model = CNN()   
if cuda_flag:
    model = CNN().cuda()

print(model)

# png = pd.read_csv('png.csv') 
# # 取出 x1...x784 data
# png_pic = png.drop(['label'], axis=1, inplace=False)
# png_y = png['label']
# #print(len(train_x))
# plt.figure(num='astronaut',figsize=(10,10))  #创建一个名为astronaut的窗口,并设置大小 
# for i in range(len(png_pic)):
#   png1=png_pic[i:i+1].values
#   png1=png1.reshape(28,28)
#   plt.subplot(5,5,i+1)     #将窗口分为5行5列25子图，则可显示25幅图片
#   plt.title(png_y[i])   #图片标题
#   plt.imshow(png1,cmap='gray')
# plt.show()



# Loss and Optimizer
loss_f = nn.CrossEntropyLoss()  # use cross entropy cost function
optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  # use Adam optimizer

#讀取測試資料
test = pd.read_csv('test_data.csv') 

#讀取訓練資料
train = pd.read_csv('train_data.csv') 
# 取出 x1...x784 data
train_x = train.drop(['label'], axis=1, inplace=False)

# #取出 y label
train_y = train['label']

# print(train_y.shape[0])
# one_hot_label = np.zeros(shape=(train_y.shape[0],25))##生成全0矩陣
# one_hot_label[np.arange(0,train_y.shape[0]),train_y] = 1.0 ##相應標籤位置置1
# train_yhot = torch.from_numpy(one_hot_label).float()

#將features (影像特徵值) 標準化 資料落在 0-1之間
test_tensor = torch.from_numpy(test.values).float() 
train_tensor = torch.from_numpy(train_x.values).float() 

#將features (影像特徵值)轉換為4維矩陣
train_tensor =train_tensor.reshape(-1,1,28,28)
test_tensor = test_tensor.reshape(-1,1,28,28)

# # 訓練資料集
torch_train = Data.TensorDataset(train_tensor, torch.from_numpy(train_y.values))
 
alltrain_loader = Data.DataLoader(torch_train, batch_size=batch_size, shuffle=True)

valid_corr=[]
tran_loss=[]
tran_corr=[]
loss_r=0.5

for tb in range(train_batch):
  # 依split_rate 將訓練資料資料集 => 分成訓練資料及驗證資料
    train_size = int(split_rate * len(torch_train))
    valida_size = len(torch_train) - train_size

    train_dataset,valida_dataset=torch.utils.data.random_split(torch_train,(train_size,valida_size))

    train_loader = Data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    valida_loader = Data.DataLoader(valida_dataset, batch_size=batch_size, shuffle=True)
#   print(len(train_loader))
#   print(len(valida_loader))
    curr_loss=[]
#   if os.path.exists(PATH):
#     model.load_state_dict(torch.load(PATH))
 # 訓練資料 Model 80%
    for epoch in range(num_epochs):
        curr_loss=[]
        model.train()
        if cuda_flag:
            model.train().cuda()  
        for i, (x, y) in enumerate(train_loader):
## mages = Variable(images.view(-1, 1, 28, 28)) 
#         b_x = Variable(x.view(-1, 1, 28, 28))
            b_x = Variable(x)
            b_y = Variable(y)
            if cuda_flag:
                b_x = Variable(x).cuda()
                b_y = Variable(y).cuda()

        # Forward + Backward + Optimize
            optimizer.zero_grad()
            output = model(b_x)
            loss = loss_f(output, b_y) # this loss_f will convert labels by one-hot encoding
            loss.backward()
       
        # Set parameters to be updated.
            optimizer.step()
            curr_loss.append(loss.item()) #save loss for epoch
            if (i+1) % 100 == 0:
                tran_loss.append(loss.item()) 
                correct = torch.sum(torch.argmax(output,dim=1)==b_y) # count the correct classification
                tran_corr.append((correct.item()/batch_size) *100)
                print ('Epoch: %d  [%d/%d], Batch: [%d/%d], Loss: %.4f, Accuracy: %.2f'
#                     % (tb+1, epoch+1, num_epochs, i+1, len(train_loader)//batch_size, loss.item(), correct.item()/batch_size))
    
# 驗證資料 Model 20%
        model.eval()
#     print(loss.item())
#     print(curr_loss)
#     print(len(curr_loss))
        with torch.no_grad(): # disable auto-grad
            correct=0
            total=0
            for x, y in valida_loader:
#             x = Variable(x.view(-1, 1, 28, 28))
                x = Variable(x)
                y = Variable(y)
                if cuda_flag:
                    x = Variable(x).cuda()
                    y = Variable(y).cuda()
                output = model(x)
                _,predicted=torch.max(output.data,1)
                total += y.size(0)
                correct += (predicted == y).sum()
            print ('valid data  Accuracy: % d %%' % (100.0 * correct/total))
            aa=correct.cpu()
            valid_corr.append(100.0 * (aa.numpy()/total))
        
        curr_loss_avg = np.mean(curr_loss) 
        print('loss_avg',curr_loss_avg)
#將訓練中loss最小的狀態保存  
#     if curr_loss_avg < loss_r:
#       loss_r=curr_loss_avg
#       print(loss_r)
#       torch.save(model.state_dict(), PATH)  
 
print(curr_loss_avg)  
#畫圖
loss_avg = np.mean(tran_loss)
corr_avg = np.mean(tran_corr)
valid_avg = np.mean(valid_corr)
print(loss_avg,corr_avg,valid_avg)
print('minloss  %.9f' %(loss_r))
show_train_history('train_loss',tran_loss)
show_train_history('train_correct',tran_corr)
show_train_history('valid_correct',valid_corr)


#model.load_state_dict(torch.load(PATH))
# # 驗證全部資料 
model.eval()
with torch.no_grad(): # disable auto-grad
    correct=0
    total=0
    for x, y in alltrain_loader:
        x = Variable(x)
        y = Variable(y)
        if cuda_flag:
            x = Variable(x).cuda()
            y = Variable(y).cuda()
        output = model(x)
        _,predicted=torch.max(output.data,1)
        total += y.size(0)
        correct += (predicted == y).sum()
    print ('all test data valid Accuracy: % d %%' % (100* correct/total))

model.eval()
with torch.no_grad(): # disable auto-grad
# # 將真正測試集餵給model 做預測
#     x = Variable(test_tensor).cuda()
#      y = Variable(y).cuda()
#     test_pred = model(x).cuda()
    
    if cuda_flag:
        test_pred = model(Variable(test_tensor).cuda())
    else:
        test_pred = model(test_tensor)

#     labelout = torch.max(test_pred, 1)[1].data.numpy()
    labelout = torch.argmax(test_pred,dim=1)
    outputs = labelout.cpu()
    print(outputs)
# #將預測結果轉成pandas 資料格式並寫入 CSV檔，最後將此結果上傳Kaggle
    res = pd.DataFrame({'samp_id': range(1, 7173), 'label': outputs})
    res.to_csv('test_sub.csv', index=False)
    
tEnd = time.time()#計時結束
print('It cost %f sec' % (tEnd - tStart)) #列印花費時間